import difflib
import re
import torch
from transformers import AutoTokenizer, T5ForConditionalGeneration
# initialize the model and tokenizer
tokenizer = None
model = None
tokenizer = None
model = None
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AutoTokenizer.from_pretrained("grammarly/coedit-large")
model = T5ForConditionalGeneration.from_pretrained("grammarly/coedit-large").to(device)
print(f"COEDIT Model and tokenizer loaded. Running on device: {device}")
    
def fix_grammar(text: str, tokenizer, model, device) -> str:
    prompt = "Fix grammar: " + text
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)
    outputs = model.generate(inputs.input_ids, max_length=64)
    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return output_text

def split_text_into_chunks(text: str, tokenizer, max_tokens: int = 64) -> list:
    """
    T√°ch text th√†nh c√°c chunk nh·ªè d·ª±a theo c√¢u, sao cho m·ªói chunk kh√¥ng v∆∞·ª£t qu√° max_tokens.
    S·ª≠ d·ª•ng regex ƒë·ªÉ t√°ch c√¢u d·ª±a tr√™n d·∫•u k·∫øt th√∫c c√¢u (., !, ?).
    """
    # T√°ch theo c√°c d·∫•u k·∫øt th√∫c c√¢u (bao g·ªìm c·∫£ kho·∫£ng tr·∫Øng ph√≠a sau)
    sentences = re.split(r'(?<=[.!?])\s+', text)
    chunks = []
    current_chunk = ""
    for sentence in sentences:
        test_chunk = current_chunk + (" " if current_chunk else "") + sentence
        tokens = tokenizer.tokenize(test_chunk)
        if len(tokens) <= max_tokens:
            current_chunk = test_chunk
        else:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = sentence
    if current_chunk:
        chunks.append(current_chunk)
    return chunks

def annotate_differences(original_text: str, corrected_text: str) -> str:
    original_words = original_text.split()
    corrected_words = corrected_text.split()

    seq_matcher = difflib.SequenceMatcher(None, original_words, corrected_words)
    opcodes = seq_matcher.get_opcodes()

    def get_word_positions(text: str):
        positions = []
        for match in re.finditer(r'\S+', text):
            start, end = match.span()
            positions.append((match.group(), start, end))
        return positions

    original_positions = get_word_positions(original_text)
    annotated_text = ""
    last_index = 0

    for tag, i1, i2, j1, j2 in opcodes:
        if tag == "equal":
            if i1 < len(original_positions):
                segment_start = original_positions[i1][1]
                segment_end = original_positions[i2 - 1][2] if (i2 - 1) < len(original_positions) else len(original_text)
                annotated_text += original_text[last_index:segment_start]
                annotated_text += original_text[segment_start:segment_end]
                last_index = segment_end
        elif tag in ["replace", "delete"]:
            if i1 < len(original_positions):
                segment_start = original_positions[i1][1]
                segment_end = original_positions[i2 - 1][2] if (i2 - 1) < len(original_positions) else len(original_text)
                error_segment = original_text[segment_start:segment_end]
                suggestion = " ".join(corrected_words[j1:j2])
                annotated_text += original_text[last_index:segment_start]
                annotated_text += (
                    f"<span class='error-block' onclick='showSuggestion(this)' data-suggestion='{suggestion}'>{error_segment}</span>"
                )
                last_index = segment_end
        elif tag == "insert":
            suggestion = " ".join(corrected_words[j1:j2])
            annotated_text += f"<span class='suggestion'>{suggestion}</span>"

    annotated_text += original_text[last_index:]
    return annotated_text


def process_document(document_text: str, tokenizer, model, device, max_tokens: int = 64) -> str:    
    """
    X·ª≠ l√Ω m·ªôt t√†i li·ªáu:
      - T√°ch t√†i li·ªáu th√†nh c√°c ƒëo·∫°n d·ª±a tr√™n c√°c k√Ω t·ª± xu·ªëng d√≤ng li√™n ti·∫øp,
        b·∫±ng c√°ch s·ª≠ d·ª•ng bi·ªÉu th·ª©c ch√≠nh quy c√≥ nh√≥m b·∫Øt ƒë·ªÉ gi·ªØ l·∫°i c√°c k√Ω t·ª± xu·ªëng d√≤ng g·ªëc.
      - V·ªõi m·ªói ƒëo·∫°n (nh·ªØng ƒëo·∫°n kh√¥ng ph·∫£i l√† ch·ªâ ch·ª©a k√Ω t·ª± xu·ªëng d√≤ng),
        n·∫øu s·ªë token v∆∞·ª£t qu√° max_tokens th√¨ t√°ch th√†nh c√°c chunk nh·ªè v√† x·ª≠ l√Ω t·ª´ng ph·∫ßn,
        c√≤n l·∫°i th√¨ x·ª≠ l√Ω tr·ª±c ti·∫øp.
      - Gh√©p l·∫°i k·∫øt qu·∫£ ƒë√£ annotate m√† v·∫´n gi·ªØ nguy√™n ƒë·ªãnh d·∫°ng ban ƒë·∫ßu c·ªßa ng∆∞·ªùi d√πng (bao g·ªìm tab, kho·∫£ng tr·∫Øng, newlines).
    """
    # D√πng capturing group ƒë·ªÉ t√°ch c√°c ƒëo·∫°n v√† gi·ªØ l·∫°i delimiter (c√°c d√≤ng xu·ªëng li√™n ti·∫øp, c√≥ th·ªÉ c√≥ kho·∫£ng tr·∫Øng v√† tab)
    segments = re.split(r'(\n\s*\n)', document_text)
    annotated_segments = []
    
    for segment in segments:
        # N·∫øu segment ch·ªâ ch·ª©a c√°c k√Ω t·ª± xu·ªëng d√≤ng (v√† kho·∫£ng tr·∫Øng) th√¨ gi·ªØ nguy√™n
        if re.fullmatch(r'\n\s*\n', segment):
            annotated_segments.append(segment)
        else:
            # Gi·ªØ nguy√™n c·∫•u tr√∫c ban ƒë·∫ßu (kh√¥ng .strip() ƒë·ªÉ b·∫£o to√†n tab, kho·∫£ng tr·∫Øng ·ªü ƒë·∫ßu/ƒëu√¥i)
            text_segment = segment
            tokens = tokenizer.tokenize(text_segment)
            if len(tokens) > max_tokens:
                chunks = split_text_into_chunks(text_segment, tokenizer, max_tokens)
            else:
                chunks = [text_segment]
            
            annotated_chunks = []
            for chunk in chunks:
                corrected_chunk = fix_grammar(chunk, tokenizer, model, device)
                annotated_chunk = annotate_differences(chunk, corrected_chunk)
                annotated_chunks.append(annotated_chunk)
            # Gh√©p l·∫°i c√°c chunk x·ª≠ l√Ω c·ªßa ƒëo·∫°n ƒë√≥ (gi·ªØa c√°c chunk m√¨nh n·ªëi b·∫±ng m·ªôt kho·∫£ng tr·∫Øng ƒë∆°n)
            annotated_text = "".join(annotated_chunks)
            annotated_segments.append(annotated_text)
    
    # Gh√©p l·∫°i to√†n b·ªô c√°c segment theo ƒë√∫ng th·ª© t·ª± ban ƒë·∫ßu
    return "".join(annotated_segments)

def wrap_words_with_click(text: str) -> str:
    words = text.split()
    return ' '.join(
        f"<span class='word' onclick='scrollToWord({i})'>{w}</span>"
        for i, w in enumerate(words)
    )

def annotated_html_with_ids(original_text: str, annotated_html: str) -> str:
    original_words = original_text.split()
    for i, word in enumerate(original_words):
        annotated_html = re.sub(
            rf"(<span class='(error|suggestion)'[^>]*?>){re.escape(word)}(</span>)",
            rf"\1<span id='suggestion-word-{i}'>\2</span>\3",
            annotated_html,
            count=1
        )
    return annotated_html

def postprocess_annotated_html(annotated_html: str) -> str:
    # Normalize line endings
    normalized = annotated_html.replace('\r\n', '\n')
    # Replace single newlines inside paragraphs with space
    normalized = re.sub(r'(?<!\n)\n(?!\n)', ' ', normalized)
    # Split into paragraphs on double newlines
    paragraphs = normalized.strip().split('\n\n')
    # Wrap with <p> and add spacing
    return ''.join(f"<p style='margin-bottom: 0.75em'>{p.strip()}</p>" for p in paragraphs if p.strip())

async def get_annotated_fixed_essay(answer: str) -> str:
    original_text = answer.strip()
    annotated_html = process_document(original_text, tokenizer, model, device, max_tokens=64)
    annotated_html = annotated_html_with_ids(original_text, annotated_html)
    # üëâ Final post-processing before returning
    annotated_html = postprocess_annotated_html(annotated_html)

    return annotated_html